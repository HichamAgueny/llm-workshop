## ğŸ§  Workshop Overview
This repository contains hands-on materials for fine-tuning and deploying **LLaMA-based Large Language Models (LLMs)** for **summarization** and **question answering (QA)**. 
The workshop is designed for execution on **HPC (High-Performance Computing) systems**, with support for both single and multi-GPU configurations on a single node.


This workshop includes:

- âœ… Fine-tuning **LLaMA models** for summarization (e.g. XSum) and QA (e.g. SQuAD)
- âœ… Running inference to generate summaries and answers
- âœ… Utilizing **single-GPU or multi-GPU setups** setup
- âœ… Executing everything on **HPC environments** with cluster tools (e.g., SLURM)
- âœ… Monitoring the GPU usage

> ğŸ“ Note: All datasets are assumed to be clean and stored in the `data/` directory. No pre-processing required.

## ğŸ“ Repository Structure
llm-workshop/

â”œâ”€â”€ container/ # Environment & singularity container

â”œâ”€â”€ data/ # Clean, ready-to-use datasets for summarization and QA

â”œâ”€â”€ download_xsum.txt # Optional script to download pre-cleaned datasets

â”œâ”€â”€ exercise/ # Guided notebooks and exercises

â”œâ”€â”€ fine-tuning-multigpu/ # Multi-GPU fine-tuning Example

â”œâ”€â”€ fine-tuning-singlegpu/ # Single-GPU fine-tuning Example

â”œâ”€â”€ inference/ # Scripts for inference (summarization and QA)

â”œâ”€â”€ install.sh # Setup script for HPC environments

â”œâ”€â”€ tools/ # Utility functions for GPU monitoring and Jobs

â”œâ”€â”€ Test/ # Simple GPU test (e.g. CUDA availability)

â””â”€â”€ README.md # Project overview and instructions
